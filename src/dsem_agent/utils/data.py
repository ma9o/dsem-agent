from pathlib import Path

from dotenv import load_dotenv

from dsem_agent.utils.config import get_config

load_dotenv(Path(__file__).parent.parent.parent.parent / ".env")

DATA_DIR = Path(__file__).parent.parent.parent.parent / "data"
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"
QUERIES_DIR = DATA_DIR / "queries"
TRAINING_DIR = DATA_DIR / "training"



def get_orchestrator_chunk_size() -> int:
    """Get chunk size for stage 1 orchestrator."""
    return get_config().stage1_structure_proposal.chunk_size


def get_worker_chunk_size() -> int:
    """Get chunk size for stage 2 workers."""
    return get_config().stage2_workers.chunk_size


def get_sample_chunks() -> int:
    """Get number of sample chunks for stage 1 structure proposal."""
    return get_config().stage1_structure_proposal.sample_chunks


# Module-level defaults (evaluated at import time from config)
CHUNK_SIZE = get_orchestrator_chunk_size()
SAMPLE_CHUNKS = get_sample_chunks()


def load_lines(path: Path) -> list[str]:
    """Load individual lines from a preprocessed file."""
    with path.open() as f:
        return [line.strip() for line in f if line.strip()]


def load_text_chunks(path: Path, chunk_size: int | None = None) -> list[str]:
    """
    Load text chunks from a preprocessed file.

    Each chunk is a group of contiguous lines joined by newlines.

    Args:
        path: Path to preprocessed file (one record per line)
        chunk_size: Lines per chunk (default: CHUNK_SIZE from .env)

    Returns:
        List of chunks, where each chunk is multiple lines joined together
    """
    chunk_size = chunk_size or CHUNK_SIZE
    lines = load_lines(path)

    chunks = []
    for i in range(0, len(lines), chunk_size):
        chunk_lines = lines[i : i + chunk_size]
        chunks.append("\n".join(chunk_lines))

    return chunks


def sample_chunks(
    input_file: Path,
    n: int,
    seed: int | None = None,
    chunk_size: int | None = None,
) -> list[str]:
    """Sample n chunks evenly spaced across the input file with jitter.

    Args:
        input_file: Path to preprocessed file
        n: Number of chunks to sample
        seed: Random seed for reproducibility
        chunk_size: Lines per chunk (default: from config)

    Returns:
        List of sampled chunks
    """
    import random

    chunks = load_text_chunks(input_file, chunk_size=chunk_size)

    if seed is not None:
        random.seed(seed)

    n = min(n, len(chunks))

    if n >= len(chunks):
        return chunks

    # Evenly space the samples across the dataset
    # Add small random jitter within each segment to avoid predictable sampling
    segment_size = len(chunks) / n
    sampled = []
    for i in range(n):
        segment_start = int(i * segment_size)
        segment_end = int((i + 1) * segment_size)
        # Pick randomly within this segment
        idx = random.randint(segment_start, segment_end - 1)
        sampled.append(chunks[idx])

    return sampled


def get_latest_preprocessed_file(
    directory: Path | None = None,
    exclude: set[str] | None = None,
) -> Path | None:
    """
    Find the most recently modified .txt file in the processed directory.

    Args:
        directory: Directory to search (default: data/processed/)
        exclude: Set of filenames to exclude (e.g., script outputs)

    Returns:
        Path to latest file, or None if no files found
    """
    search_dir = directory or PROCESSED_DIR
    exclude = exclude or set()
    txt_files = [f for f in search_dir.glob("*.txt") if f.name not in exclude]

    if not txt_files:
        return None

    # Sort by modification time, newest first
    return max(txt_files, key=lambda p: p.stat().st_mtime)


def resolve_input_path(filename: str | None = None) -> Path:
    """
    Resolve input path from filename or find latest preprocessed file.

    Args:
        filename: Optional filename (just name, not full path) in preprocessed dir.
                  If None, returns the latest preprocessed file.

    Returns:
        Full path to the input file

    Raises:
        FileNotFoundError: If no matching file found
    """
    if filename:
        path = PROCESSED_DIR / filename
        if not path.exists():
            raise FileNotFoundError(f"File not found: {path}")
        return path

    latest = get_latest_preprocessed_file()
    if not latest:
        raise FileNotFoundError(f"No preprocessed files found in {PROCESSED_DIR}")

    return latest


def resolve_query_path(filename: str) -> Path:
    """
    Resolve query file path from filename.

    Args:
        filename: Filename (mnemonic name like 'smoking-cancer.txt') in test-queries dir.
                  Extension is optional - will try .txt and .md if not provided.

    Returns:
        Full path to the query file

    Raises:
        FileNotFoundError: If no matching file found
    """
    # Try exact match first
    path = QUERIES_DIR / filename
    if path.exists():
        return path

    # Try with extensions if not provided
    if not path.suffix:
        for ext in [".txt", ".md"]:
            path_with_ext = QUERIES_DIR / f"{filename}{ext}"
            if path_with_ext.exists():
                return path_with_ext

    raise FileNotFoundError(f"Query file not found: {filename} in {QUERIES_DIR}")


def load_query(filename: str) -> str:
    """
    Load query content from file.

    Args:
        filename: Filename in test-queries dir (e.g., 'smoking-cancer')

    Returns:
        Query text content
    """
    path = resolve_query_path(filename)
    return path.read_text().strip()


def list_queries() -> list[str]:
    """List available query files in test-queries directory."""
    return [p.name for p in QUERIES_DIR.glob("*") if p.is_file() and p.name != ".gitkeep"]
